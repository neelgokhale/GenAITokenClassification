{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2943e283-05d1-4080-8d70-1ac5f14bca2f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a81c06d6-2514-4e47-baff-f6d591f664be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.12\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "168d6ff0-ed59-47da-8050-42a1841abffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers.utils import send_example_telemetry\n",
    "send_example_telemetry(\"token_classification_notebook\", framework=\"pytorch\")\n",
    "\n",
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2cd9e946-d518-4e5a-9be6-c61e89177c44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc7a72d477f4706a0311755bd14d907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f84ca51-c762-4e6a-a23c-f641e6935261",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb5f60f-d637-490a-9756-777632c12ff7",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af0b1757-be7a-45c4-b231-5f9e8f4c36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b4fc94c-58bf-4a92-9ec6-4d26e633066f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 3394\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1009\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'ner_tags'],\n",
       "        num_rows: 1287\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "807c1099-5e40-4917-bdec-a9eb490e84c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'B-corporation',\n",
       " 'I-corporation',\n",
       " 'B-creative-work',\n",
       " 'I-creative-work',\n",
       " 'B-group',\n",
       " 'I-group',\n",
       " 'B-location',\n",
       " 'I-location',\n",
       " 'B-person',\n",
       " 'I-person',\n",
       " 'B-product',\n",
       " 'I-product']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = datasets[\"train\"].features[\"ner_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d8fc165-e3d6-4a3c-86dd-b8f919c828b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2588</td>\n",
       "      <td>[NASCAR, 's, Popular, Regional, Series, ,, ACT, Return, to, Help, Celebrate, 25th, ..., -, RaceDayCT, http://t.co/fYjrBS1QW0, #nascar]</td>\n",
       "      <td>[B-corporation, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>392</td>\n",
       "      <td>[about, to, get, dress, hangin, wit, the, big, sis, Abby, tonite]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-person, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2187</td>\n",
       "      <td>[@XboxSupport, after, making, any, changes, to, an, avatar, whole, system, chugs, along, and, when, booting, up, a, game, it, freeze, the, system, up, ., 250g, 360-s]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1261</td>\n",
       "      <td>[Halo, Reach, was, a, bit, crap, tonight]</td>\n",
       "      <td>[B-product, I-product, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2050</td>\n",
       "      <td>[I, just, took, \", After, getting, trampled, at, a, Justin, Bieber, concert, ,, yoiu, wake, up, and, ...\", and, got, :, Part, 6, :), !, Try, it, :, http://tinyurl.com/26zeju5]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, B-person, I-person, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>181</td>\n",
       "      <td>[FRIDAY, Sept, ., 17, http://goo.gl/fb/83ib3]</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2231</td>\n",
       "      <td>[#MedicalJobs, CT/Rad, Tech, -, PRN, :, TX-Fort, Worth, ,, When, physicians, own, the, hospital, ,, the, latest, advances, in, medical, s, ..., http://bit.ly/cVPhNE]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3276</td>\n",
       "      <td>[Since, Saturday, night, ., http://t.co/hTdDzwfEkG]</td>\n",
       "      <td>[O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>753</td>\n",
       "      <td>[RT, @therealadamwest, :, Sunday, is, my, birthday, ., 82, years, young, !, What, should, I, wear, ?, (, don't, say, birthday, suit, !, )]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3168</td>\n",
       "      <td>[-If, you, value, your, life, ,, if, you, have, any, hope, of, seeing, tomorrow, ,, there, 's, something, you, never, EVER, put, in, a, trap, -And, what, would, that, be, ?, -ME]</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59db2565-ca6c-47cd-8af3-5772e155c11f",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2e812c-9869-4b33-ad49-db41ea9024a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8bb7cfd8-2e6b-45f7-80d5-8490e9c97afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33e08f7d-3737-4d10-aeed-c1fce596fd77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2026, 2171, 2003, 7663, 2140, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, my name is Neel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da656353-f054-49de-a18c-a71b93756ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '0', 'tokens': ['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'], 'ner_tags': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "ex = datasets[\"train\"][0]\n",
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae4a780d-5645-43e7-a3f0-ce7e2ddda4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@paulwalk', 'It', \"'s\", 'the', 'view', 'from', 'where', 'I', \"'m\", 'living', 'for', 'two', 'weeks', '.', 'Empire', 'State', 'Building', '=', 'ESB', '.', 'Pretty', 'bad', 'storm', 'here', 'last', 'evening', '.'] 27\n",
      "['[CLS]', '@', 'paul', '##walk', 'it', \"'\", 's', 'the', 'view', 'from', 'where', 'i', \"'\", 'm', 'living', 'for', 'two', 'weeks', '.', 'empire', 'state', 'building', '=', 'es', '##b', '.', 'pretty', 'bad', 'storm', 'here', 'last', 'evening', '.', '[SEP]'] 34\n"
     ]
    }
   ],
   "source": [
    "ex_tokenized = tokenizer(ex[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(ex_tokenized[\"input_ids\"])\n",
    "\n",
    "print(ex[\"tokens\"], len(ex[\"tokens\"]))\n",
    "print(tokens, len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2815e469-a7c0-4bf9-aacc-afb85ae32dc5",
   "metadata": {},
   "source": [
    "### Align and Tokenize Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e93db28b-f79e-40f1-a55d-bbf0f7b5e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if below is True, label all sub-words following the first sub-word, else assign -100 to all following sub-words\n",
    "label_all_tokens = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e8dd2e0-63f9-40ba-809a-d9fec1508503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_label(ex):\n",
    "    # tokenize all examples, truncate if exceeding max model size and accept pre-split words\n",
    "    ex_tokenized = tokenizer(ex[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(ex[\"ner_tags\"]):\n",
    "        word_ids = ex_tokenized.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            # set the labels of special tokens (CLS, SEP) that have a word id of None to -100\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # set the label of the first token of each word\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # for other tokens, either set -100 or the current label depending on the label_all_tokens flag\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "                \n",
    "            previous_word_idx = word_idx      \n",
    "        labels.append(label_ids) \n",
    "    ex_tokenized[\"labels\"] = labels\n",
    "    \n",
    "    return ex_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c22247a5-4ac1-4201-ab98-2ec0f0431dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42796d499a304210b26e0b5496299421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# apply function to all sentences in dataset, use map function\n",
    "tokenized_datasets = datasets.map(tokenize_and_align_label, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976ace98-21c9-4277-b6a0-9ba2b1b31eec",
   "metadata": {},
   "source": [
    "## Fine-tuning the `distilbert` Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66490afd-95ac-4b35-a37a-e06d2a06ea30",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"O\",\n",
    "    1: \"B-corporation\",\n",
    "    2: \"I-corporation\",\n",
    "    3: \"B-creative-work\",\n",
    "    4: \"I-creative-work\",\n",
    "    5: \"B-group\",\n",
    "    6: \"I-group\",\n",
    "    7: \"B-location\",\n",
    "    8: \"I-location\",\n",
    "    9: \"B-person\",\n",
    "    10: \"I-person\",\n",
    "    11: \"B-product\",\n",
    "    12: \"I-product\",\n",
    "}\n",
    "label2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-corporation\": 1,\n",
    "    \"I-corporation\": 2,\n",
    "    \"B-creative-work\": 3,\n",
    "    \"I-creative-work\": 4,\n",
    "    \"B-group\": 5,\n",
    "    \"I-group\": 6,\n",
    "    \"B-location\": 7,\n",
    "    \"I-location\": 8,\n",
    "    \"B-person\": 9,\n",
    "    \"I-person\": 10,\n",
    "    \"B-product\": 11,\n",
    "    \"I-product\": 12,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d161ac0f-2b60-4b7d-8600-21cc5da1d133",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# establishing the base model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=len(label_list), id2label=id2label, label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec8ac2a4-6ae7-463c-b517-4f118f11cc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilbert-finetuned-ner\"\n",
    "\n",
    "# establishing training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_name,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=4,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93686b2-128c-4748-bb74-aa1300924708",
   "metadata": {},
   "source": [
    "### Create a Data Collator to Pad Tokens and Training Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "823d2399-fefc-4ec5-b531-7a07816d27e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "# establishing data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dea2893c-d98c-41e6-86b3-accc45e775b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# establishing seqeval metric\n",
    "metric = evaluate.load(\"seqeval\")\n",
    "\n",
    "# post-processing for metric results generated\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe515ad-65e5-4557-a62c-785bfa4aeff8",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a863717-1d32-4589-922d-fe773c6983b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c262ed0-913b-4a0d-beeb-5858bf164d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='852' max='852' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [852/852 37:39, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.278026</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.250232</td>\n",
       "      <td>0.353172</td>\n",
       "      <td>0.938352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.251375</td>\n",
       "      <td>0.538897</td>\n",
       "      <td>0.353105</td>\n",
       "      <td>0.426652</td>\n",
       "      <td>0.943055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.257625</td>\n",
       "      <td>0.545332</td>\n",
       "      <td>0.373494</td>\n",
       "      <td>0.443344</td>\n",
       "      <td>0.945406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.191000</td>\n",
       "      <td>0.271600</td>\n",
       "      <td>0.544098</td>\n",
       "      <td>0.371640</td>\n",
       "      <td>0.441630</td>\n",
       "      <td>0.945535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/pytorch/lib/python3.10/site-packages/seqeval/metrics/v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=852, training_loss=0.13877330923304312, metrics={'train_runtime': 2262.0758, 'train_samples_per_second': 6.002, 'train_steps_per_second': 0.377, 'total_flos': 184068639256200.0, 'train_loss': 0.13877330923304312, 'epoch': 4.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f362a9-0ed6-4e8b-89fa-c5a2688c198c",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6e9ac68b-b432-40cd-8d1f-965a49297f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.27652886509895325,\n",
       " 'eval_precision': 0.5822050290135397,\n",
       " 'eval_recall': 0.2789620018535681,\n",
       " 'eval_f1': 0.37719298245614036,\n",
       " 'eval_accuracy': 0.9410884528237357,\n",
       " 'eval_runtime': 70.6462,\n",
       " 'eval_samples_per_second': 18.218,\n",
       " 'eval_steps_per_second': 1.147,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "16e15b1d-e891-4a09-ad9e-0755e6f9b8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'corporation': {'precision': 0.35714285714285715,\n",
       "  'recall': 0.29411764705882354,\n",
       "  'f1': 0.3225806451612903,\n",
       "  'number': 34},\n",
       " 'creative-work': {'precision': 0.2727272727272727,\n",
       "  'recall': 0.02857142857142857,\n",
       "  'f1': 0.05172413793103448,\n",
       "  'number': 105},\n",
       " 'group': {'precision': 0.36363636363636365,\n",
       "  'recall': 0.20512820512820512,\n",
       "  'f1': 0.2622950819672131,\n",
       "  'number': 39},\n",
       " 'location': {'precision': 0.4375,\n",
       "  'recall': 0.5675675675675675,\n",
       "  'f1': 0.4941176470588235,\n",
       "  'number': 74},\n",
       " 'person': {'precision': 0.7695961995249406,\n",
       "  'recall': 0.6893617021276596,\n",
       "  'f1': 0.7272727272727273,\n",
       "  'number': 470},\n",
       " 'product': {'precision': 0.4827586206896552,\n",
       "  'recall': 0.12280701754385964,\n",
       "  'f1': 0.1958041958041958,\n",
       "  'number': 114},\n",
       " 'overall_precision': 0.6606260296540363,\n",
       " 'overall_recall': 0.4796650717703349,\n",
       " 'overall_f1': 0.5557865557865559,\n",
       " 'overall_accuracy': 0.9504736474028864}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32062346-99f2-4904-b864-4ff6a726dbbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/neelgokhale/distilbert-finetuned-ner/tree/main/'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(\n",
    "    commit_message=\"training: 4 epochs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d991bad7-4a0f-4a0b-bba8-2c0749c514ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/workbench-notebooks:m111"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
